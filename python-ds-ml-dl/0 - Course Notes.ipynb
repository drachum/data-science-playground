{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Statistics and Probability\n",
    "\n",
    "## PDF and PMF\n",
    "\n",
    "PDF => probability density function => probability of an interval\n",
    "\n",
    "PMF => integral of PDF => probability of one event\n",
    "\n",
    "\n",
    "## Types of Distributions\n",
    "\n",
    "### PDFs\n",
    "\n",
    "1. Uniform => np.random.uniform\n",
    "2. Normal => np.random.normal\n",
    "3. Exponential => from scipy import expon\n",
    "\n",
    "\n",
    "### PMFs\n",
    "\n",
    "1. Binomial\n",
    "2. Poisson\n",
    "\n",
    "\n",
    "## Moments\n",
    "\n",
    "1. mean\n",
    "2. variance\n",
    "3. skew => how lopsided is the distribution, how is the tail of it? (longer tail to the left have negative skew)\n",
    "4. kurtosis => how thick is the distribution? Higher peaks have higher kurtosis\n",
    "\n",
    "\n",
    "## Covariance and Correntlation\n",
    "\n",
    "1. Covariance => a measure of how two variables vary together\n",
    "2. Correlation => covariance divided by the standard deviation. It can be used to decide which experiments to go forward with\n",
    "\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "1. Probability of P(A|B) can be very different from P(B|A)... Drug test example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Predictive Models\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "1. Fits a line to a data set\n",
    "2. $r^2$ measures the fraction of the variance that is captue by the model\n",
    "\n",
    "## Polynomial Regression\n",
    "\n",
    "1. Uses a polynomial to fit the model\n",
    "2. Beware of overfitting\n",
    "\n",
    "## Multivariate Regression\n",
    "\n",
    "1. Betas model, each feature an independent term on the model and each beta a coefficient\n",
    "\n",
    "## Multilevel Models\n",
    "\n",
    "1. Try to get a sense of the many hierarchies present when creating models\n",
    "2. Look at different layers of effects\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Machine Learning with Python\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "1. Model isn't given any answers to learn from\n",
    "2. Use when I'm looking for latent variable, where I don't know what I'm looking for\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "1. Model learns from correct answers given to it\n",
    "2. Split data between train and test: avoid overfitting\n",
    "3. k-fold test validation\n",
    "\n",
    "## Machine Learning Methods\n",
    "\n",
    "### Bayesian\n",
    "\n",
    "1. Uses Bayes theorem and ignores possible correlations between variables to generate a model\n",
    "\n",
    "### k-means\n",
    "\n",
    "1. Splits data into k groups that are closest to centroids\n",
    "2. Choosing K is a challenge, choose the one which minimizes error metric\n",
    "3. It doesn't gives labels for the groups\n",
    "\n",
    "### Decision trees\n",
    "\n",
    "1. Entropy: measure of a dataset disorder\n",
    "2. A tree formed by the features that arrives at a decision (class)\n",
    "3. Ramdom forest: many random decision trees vote for ther final (bootstrap agreggativng: bagging).\n",
    "    3.a. Very important: each tree must be trained with random sampling\n",
    "\n",
    "### Ensemble Learning\n",
    "\n",
    "1. Bagging: bootstrap aggregating => train many models with random samples from trainiing and vote\n",
    "2. Boosting: each successive model address weak points from previous ones\n",
    "3. Bucket of models: train different models and pick the best one\n",
    "4. Stacking: combine multiple models\n",
    "\n",
    "### SVM\n",
    "\n",
    "1. Use SVC normally\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Recommender Systems\n",
    "\n",
    "Systems that recommend options for a user based on past behaviour\n",
    "\n",
    "## User Based Collaborative Filtering\n",
    "\n",
    "1. Create a marix with things user bought\n",
    "2. Compute similarities between users\n",
    "3. Find user similar to me\n",
    "4. Recommend things that user bought to me\n",
    "\n",
    "Problems:\n",
    "    \n",
    "1. People change\n",
    "2. There are more poeople than things\n",
    "3. People do bad things, noise in data\n",
    "\n",
    "\n",
    "## Item Based Collaborative Filtering\n",
    "\n",
    "Solves the problems with user based filtering.\n",
    "\n",
    "1. Find every pair of movies\n",
    "2. Measure the similarity of ratings across users who watched both\n",
    "3. Sor by movie, then similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - More Machine Learning Techniques\n",
    "\n",
    "## K-Nearest Neighbors\n",
    "\n",
    "1. Compute new data distance to know data\n",
    "2. all vote for them\n",
    "3. Class is the min distance\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "1. Preserve the variation reducing the number of dimentions\n",
    "2. Use PCA\n",
    "    a. Reduce dimensions projectiong data into the direction of eingenvectors, that is, planes in the higher dimension space\n",
    "    b. Normally yses singular value decomposition (SVD)\n",
    "\n",
    "## Data Warehousing\n",
    "\n",
    "1. Centralized DB with info from many sources\n",
    "2. ETL\n",
    "    a. Extract: extract raw data periodically from sources\n",
    "    b. Transform: transform data into wanted structure\n",
    "    c. Load: load data into new schema\n",
    "3. Transform step is a big problem with big data\n",
    "4. ELT:\n",
    "    a. Extract and load as it is\n",
    "    b. Use power of hadoop to transform it on the go\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "1. Q-Learning\n",
    "    a. A set of environmental states a\n",
    "    b. A set of possible actions ins those states s\n",
    "    c. Value of each state action Q\n",
    "    d. Start with Q = 0. Decease or increase it based on consequences of actions in states\n",
    "2. This is a Markov decision process (can be modelled as)\n",
    "3. Dynamic program\n",
    "    a. Calculate possible solutions and store then, to just consult then at another time\n",
    "    b. this occurs by breaking a problem in smaller subproblmes\n",
    "\n",
    "## Measuring Classifiers\n",
    "\n",
    "1. Confusion Matrix: Tool to understand true positives and true negatives\n",
    "2. Metrics:\n",
    "    a) Recall: percentage of negatives wrongly predicted => when we worry too much about false negatives, fraud detections\n",
    "    b) Precision: percentage of relevant results => when we care about false positives, like cancer detection\n",
    "    c) Specificity, Sensitivity\n",
    "    d) RMSE\n",
    "    e) ROC curve\n",
    "    f) AUC\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Dealing with Real World Data\n",
    "\n",
    "## Bias X Variance\n",
    "\n",
    "1. Bias => how far from target\n",
    "2. Variance => how scattered across a point\n",
    "3. Usually we have to choose one of the other\n",
    "4. Error = bias^2 + variance\n",
    "5. k-fold cross validations helps to avoid overfitting\n",
    "\n",
    "## Cleaning Data\n",
    "\n",
    "6. Always check if data is clean and consistent => Garbage In, Garbage Out\n",
    "7. Always normalize numerical data: sci-kit learning preprocessing module and PCA whitening\n",
    "8. How to detect outliers? Number of std out from mean\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "9. Feature Engineering is the art of machine learning, here lies the most knowledge intensive process\n",
    "10. Curse of dimensionality: try to have less features as possible, PCA and k-means can help with this\n",
    "\n",
    "## Imputation\n",
    "\n",
    "1. Mean Inputation => easy to do, results are terrible. Sometimes using median can be better\n",
    "2. Drop data => almost anything is better\n",
    "3. Machine Learning => build a model to input data to what I want, to replace the inconsistent row\n",
    "\n",
    "## Unbalance Data\n",
    "\n",
    "1. This problem occurs more with neural networks\n",
    "2. Oversampling: duplicate samples from minority cases at random\n",
    "3. Undersampling: throw off data from the majority case\n",
    "4. SMOTE: Synthetic Minority Over-sampling TEchnique\n",
    "5. Use knn to generate samples from the minority class\n",
    "6. Adjust thresholds to compensate from unbalanced data\n",
    "\n",
    "## Other Techniques\n",
    "\n",
    "1. Binning: take a quantity and buckte it => 0-20 years, 20-40, ...\n",
    "2. Transforming: transforming => sqrt of input, input squared, log of input, and others. Requires knowledge of domain\n",
    "3. Encoding: one-hot encoding, ie. Very used in deep learning\n",
    "4. Scaling/Normalizing: scale input better to a range more suited to problem\n",
    "5. Shuffling: shuffle data to avoid any temporal correlations that may be present in input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Apache Spark\n",
    "\n",
    "## Introduction\n",
    "\n",
    "1. Large scale file processing\n",
    "2. Distributed, scalable\n",
    "3. Faster than Hadoop MapReduce\n",
    "4. Resilient Distributed Dataset\n",
    "\n",
    "## Resilient Distributed Dataset\n",
    "\n",
    "1. SparkContext: creates RDD\n",
    "2. Can read from a great variety of files (databases)\n",
    "3. You can transform RDDS in some new values using map, filter, transform, ...\n",
    "\n",
    "## MLlib\n",
    "\n",
    "1. Special types: Vector, LabeledPoint, Rating\n",
    "2. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Experimental Design / ML in the Real World\n",
    "\n",
    "## Deploying models to Real time systems\n",
    "\n",
    "1. Train model periodically\n",
    "2. Push model to a web service\n",
    "3. Apps call this webservice\n",
    "4. SKLearn has tools to export models\n",
    "5. Another approach: run batches of predictions and store results in DB\n",
    "\n",
    "## A/B Testing\n",
    "\n",
    "1. Test the performance of some change relative of how it was before\n",
    "2. Try to test one thing at each time, otherwise will get lost\n",
    "3. Variance is your enemy: always have enough data to see if things are really what they seem to be\n",
    "4. Try to measure variables with less variance\n",
    "\n",
    "## T-tests and P-values\n",
    "\n",
    "1. Say to us if results are real or just random variance\n",
    "2. T-tests are a measure of difference betweeen two sets in terms of standard errors. Assume normal variance, see others for other data\n",
    "3. p-vlaue: probability of confirming the null hipothesis. Lower p-value, the better\n",
    "\n",
    "## Gotchas\n",
    "\n",
    "1. Correlation does not imply causation => managers are hard to understand this\n",
    "2. Seasonal effects: weekends, holidays, Christmas, ...\n",
    "3. Selection bias => run A/A tests periodically\n",
    "4. Data pollution => outliers, robots thrashing the data, ...\n",
    "5. Attribution errors, how conversion is counted, etc...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Deep Learning and Neural Networks\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "1. Gradient descent: minimization by finding local minima\n",
    "2. Use of autodiff: optimized for few outputs => reverse mode autodiff\n",
    "3. Softmax used to generate an output probability\n",
    "\n",
    "## History of ANNs\n",
    "\n",
    "1. Inspired by neurons\n",
    "2. Cortical colunms, stacks of neurons => this is similar to GPUs\n",
    "3. Artifical Neuron: 1943\n",
    "4. LTU => linear threshold unit => 1957 => weights added to inputs\n",
    "5. Perceptron: a layer of LTUs\n",
    "6. Multi-layer perceptrons: addition of hidden layers, a deep network. Training is trickier\n",
    "7. Modern deep layer networ: replace step activation with something better, add softmax to output, train with gradient descent\n",
    "\n",
    "## Deep Learning Details\n",
    "\n",
    "1. Playground: http://playground.tensorflow.org\n",
    "2. Deep learning is trained using backpropagation\n",
    "3. Activation funtion: ReLU is used most of the times\n",
    "4. There are faster methods than backpropagation: momenutm, nesterov, adam, ...\n",
    "5. Dropout is good for fast learning\n",
    "6. Use model zoos\n",
    "\n",
    "## Tensorflow Introduction\n",
    "\n",
    "1. A library to distribute graph computing\n",
    "2. Can work on GPUs naturally\n",
    "3. When using NNs, normalize inputs with 0 mean and 1 std\n",
    "\n",
    "## Keras\n",
    "\n",
    "1. Integration with sci-kit learn\n",
    "2. KerasClassifier\n",
    "3. Slower but practical\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "1. Based on retina configuration\n",
    "2. Dropout layers are very used to avoid overfitting\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "1. Time series data, sequential data\n",
    "2. Layers have feedback from outputs\n",
    "3. When we add multiple layers, we can create \"memory celss\"\n",
    "4. We create a layer of recurrent neurons\n",
    "5. To train, we use back propagtion applied in time, besides the layers\n",
    "6. Different approaches to deal with state, LSTM, GRU cells\n",
    "7. Very hard to train, sometimes doesn't converge at all: look at previous research!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Statistics and Probability\n",
    "\n",
    "## PDF and PMF\n",
    "\n",
    "PDF => probability density function => probability of an interval\n",
    "\n",
    "PMF => integral of PDF => probability of one event\n",
    "\n",
    "\n",
    "## Types of Distributions\n",
    "\n",
    "### PDFs\n",
    "\n",
    "1. Uniform => np.random.uniform\n",
    "2. Normal => np.random.normal\n",
    "3. Exponential => from scipy import expon\n",
    "\n",
    "\n",
    "### PMFs\n",
    "\n",
    "1. Binomial\n",
    "2. Poisson\n",
    "\n",
    "\n",
    "## Moments\n",
    "\n",
    "1. mean\n",
    "2. variance\n",
    "3. skew => how lopsided is the distribution, how is the tail of it? (longer tail to the left have negative skew)\n",
    "4. kurtosis => how thick is the distribution? Higher peaks have higher kurtosis\n",
    "\n",
    "\n",
    "## Covariance and Correntlation\n",
    "\n",
    "1. Covariance => a measure of how two variables vary together\n",
    "2. Correlation => covariance divided by the standard deviation. It can be used to decide which experiments to go forward with\n",
    "\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "1. Probability of P(A|B) can be very different from P(B|A)... Drug test example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Predictive Models\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "1. Fits a line to a data set\n",
    "2. $r^2$ measures the fraction of the variance that is captue by the model\n",
    "\n",
    "## Polynomial Regression\n",
    "\n",
    "1. Uses a polynomial to fit the model\n",
    "2. Beware of overfitting\n",
    "\n",
    "## Multivariate Regression\n",
    "\n",
    "1. Betas model, each feature an independent term on the model and each beta a coefficient\n",
    "\n",
    "## Multilevel Models\n",
    "\n",
    "1. Try to get a sense of the many hierarchies present when creating models\n",
    "2. Look at different layers of effects\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Machine Learning with Python\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "1. Model isn't given any answers to learn from\n",
    "2. Use when I'm looking for latent variable, where I don't know what I'm looking for\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "1. Model learns from correct answers given to it\n",
    "2. Split data between train and test: avoid overfitting\n",
    "3. k-fold test validation\n",
    "\n",
    "## Machine Learning Methods\n",
    "\n",
    "### Bayesian\n",
    "\n",
    "1. Uses Bayes theorem and ignores possible correlations between variables to generate a model\n",
    "\n",
    "### k-means\n",
    "\n",
    "1. Splits data into k groups that are closest to centroids\n",
    "2. Choosing K is a challenge, choose the one which minimizes error metric\n",
    "3. It doesn't gives labels for the groups\n",
    "\n",
    "### Decision trees\n",
    "\n",
    "1. Entropy: measure of a dataset disorder\n",
    "2. A tree formed by the features that arrives at a decision (class)\n",
    "3. Ramdom forest: many random decision trees vote for ther final (bootstrap agreggativng: bagging).\n",
    "    3.a. Very important: each tree must be trained with random sampling\n",
    "\n",
    "### Ensemble Learning\n",
    "\n",
    "1. Bagging: bootstrap aggregating => train many models with random samples from trainiing and vote\n",
    "2. Boosting: each successive model address weak points from previous ones\n",
    "3. Bucket of models: train different models and pick the best one\n",
    "4. Stacking: combine multiple models\n",
    "\n",
    "### SVM\n",
    "\n",
    "1. Use SVC normally\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Recommender Systems\n",
    "\n",
    "Systems that recommend options for a user based on past behaviour\n",
    "\n",
    "## User Based Collaborative Filtering\n",
    "\n",
    "1. Create a marix with things user bought\n",
    "2. Compute similarities between users\n",
    "3. Find user similar to me\n",
    "4. Recommend things that user bought to me\n",
    "\n",
    "Problems:\n",
    "    \n",
    "1. People change\n",
    "2. There are more poeople than things\n",
    "3. People do bad things, noise in data\n",
    "\n",
    "\n",
    "## Item Based Collaborative Filtering\n",
    "\n",
    "Solves the problems with user based filtering.\n",
    "\n",
    "1. Find every pair of movies\n",
    "2. Measure the similarity of ratings across users who watched both\n",
    "3. Sor by movie, then similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - More Machine Learning Techniques\n",
    "\n",
    "## K-Nearest Neighbors\n",
    "\n",
    "1. Compute new data distance to know data\n",
    "2. all vote for them\n",
    "3. Class is the min distance\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "1. Preserve the variation reducing the number of dimentions\n",
    "2. Use PCA\n",
    "    a. Reduce dimensions projectiong data into the direction of eingenvectors, that is, planes in the higher dimension space\n",
    "    b. Normally yses singular value decomposition (SVD)\n",
    "\n",
    "## Data Warehousing\n",
    "\n",
    "1. Centralized DB with info from many sources\n",
    "2. ETL\n",
    "    a. Extract: extract raw data periodically from sources\n",
    "    b. Transform: transform data into wanted structure\n",
    "    c. Load: load data into new schema\n",
    "3. Transform step is a big problem with big data\n",
    "4. ELT:\n",
    "    a. Extract and load as it is\n",
    "    b. Use power of hadoop to transform it on the go\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "1. Q-Learning\n",
    "    a. A set of environmental states a\n",
    "    b. A set of possible actions ins those states s\n",
    "    c. Value of each state action Q\n",
    "    d. Start with Q = 0. Decease or increase it based on consequences of actions in states\n",
    "2. This is a Markov decision process (can be modelled as)\n",
    "3. Dynamic program\n",
    "    a. Calculate possible solutions and store then, to just consult then at another time\n",
    "    b. this occurs by breaking a problem in smaller subproblmes\n",
    "\n",
    "## Measuring Classifiers\n",
    "\n",
    "1. Confusion Matrix: Tool to understand true positives and true negatives\n",
    "2. Metrics:\n",
    "    a) Recall: percentage of negatives wrongly predicted => when we worry too much about false negatives, fraud detections\n",
    "    b) Precision: percentage of relevant results => when we care about false positives, like cancer detection\n",
    "    c) Specificity, Sensitivity\n",
    "    d) RMSE\n",
    "    e) ROC curve\n",
    "    f) AUC\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Dealing with Real World Data\n",
    "\n",
    "## Bias X Variance\n",
    "\n",
    "1. Bias => how far from target\n",
    "2. Variance => how scattered across a point\n",
    "3. Usually we have to choose one of the other\n",
    "4. Error = bias^2 + variance\n",
    "5. k-fold cross validations helps to avoid overfitting\n",
    "\n",
    "## Cleaning Data\n",
    "\n",
    "6. Always check if data is clean and consistent => Garbage In, Garbage Out\n",
    "7. Always normalize numerical data: sci-kit learning preprocessing module and PCA whitening\n",
    "8. How to detect outliers? Number of std out from mean\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "9. Feature Engineering is the art of machine learning, here lies the most knowledge intensive process\n",
    "10. Curse of dimensionality: try to have less features as possible, PCA and k-means can help with this\n",
    "\n",
    "## Imputation\n",
    "\n",
    "1. Mean Inputation => easy to do, results are terrible. Sometimes using median can be better\n",
    "2. Drop data => almost anything is better\n",
    "3. Machine Learning => build a model to input data to what I want, to replace the inconsistent row\n",
    "\n",
    "## Unbalance Data\n",
    "\n",
    "1. This problem occurs more with neural networks\n",
    "2. Oversampling: duplicate samples from minority cases at random\n",
    "3. Undersampling: throw off data from the majority case\n",
    "4. SMOTE: Synthetic Minority Over-sampling TEchnique\n",
    "5. Use knn to generate samples from the minority class\n",
    "6. Adjust thresholds to compensate from unbalanced data\n",
    "\n",
    "## Other Techniques\n",
    "\n",
    "1. Binning: take a quantity and buckte it => 0-20 years, 20-40, ...\n",
    "2. Transforming: transforming => sqrt of input, input squared, log of input, and others. Requires knowledge of domain\n",
    "3. Encoding: one-hot encoding, ie. Very used in deep learning\n",
    "4. Scaling/Normalizing: scale input better to a range more suited to problem\n",
    "5. Shuffling: shuffle data to avoid any temporal correlations that may be present in input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Apache Spark\n",
    "\n",
    "## Introduction\n",
    "\n",
    "1. Large scale file processing\n",
    "2. Distributed, scalable\n",
    "3. Faster than Hadoop MapReduce\n",
    "4. Resilient Distributed Dataset\n",
    "\n",
    "## Resilient Distributed Dataset\n",
    "\n",
    "1. SparkContext: creates RDD\n",
    "2. Can read from a great variety of files (databases)\n",
    "3. You can transform RDDS in some new values using map, filter, transform, ...\n",
    "\n",
    "## MLlib\n",
    "\n",
    "1. Special types: Vector, LabeledPoint, Rating\n",
    "2. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Experimental Design / ML in the Real World\n",
    "\n",
    "## Deploying models to Real time systems\n",
    "\n",
    "1. asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
